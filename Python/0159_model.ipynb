{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sensitive-proceeding",
   "metadata": {},
   "source": [
    "## PyTorch Convolutional Neural Network With MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constitutional-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- pytorch utility imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import math\n",
    "import functions\n",
    "\n",
    "# --- neural net imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Ft\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "victorian-change",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "geological-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- import external libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math\n",
    "from scipy import signal\n",
    "from bitstring import Bits\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading dataset\n",
    "input_folder_path = \"./data/\"\n",
    "train_df = pd.read_csv(input_folder_path+\"train_0159_7_7.csv\")\n",
    "test_df = pd.read_csv(input_folder_path+\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-arrival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16794</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16797</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16798</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16799 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  1  2  3  4  5  6  7  8  9  ...  91  92  93  94   95  96  97  98  \\\n",
       "0          0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "1          0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "2          0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "3          0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "4          0  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "...      ... .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ...  ..  ..  ..   \n",
       "16794      9  0  0  0  0  0  0  0  0  0  ...   0   0   0   0  178   0   0   0   \n",
       "16795      9  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "16796      9  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0  81   0   \n",
       "16797      9  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "16798      9  0  0  0  0  0  0  0  0  0  ...   0   0   0   0    0   0   0   0   \n",
       "\n",
       "       99  100  \n",
       "0       0    0  \n",
       "1       0    0  \n",
       "2       0    0  \n",
       "3       0    0  \n",
       "4       0    0  \n",
       "...    ..  ...  \n",
       "16794   0    0  \n",
       "16795   0    0  \n",
       "16796   0    0  \n",
       "16797   0    0  \n",
       "16798   0    0  \n",
       "\n",
       "[16799 rows x 101 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stopped-intervention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "\n",
       "[1 rows x 784 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "destroyed-treasury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16799, 101)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-rhythm",
   "metadata": {},
   "source": [
    "### Separate into labels and training images and reshape the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hydraulic-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['label'].values\n",
    "train_images = (train_df.iloc[:,1:].values).astype('float32')\n",
    "test_images = (test_df.iloc[:,:].values).astype('float32')\n",
    "\n",
    "# --- Training and Validation Split\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels,\n",
    "                                                                      stratify=train_labels, random_state=123,\n",
    "                                                                      test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "respiratory-chrome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (13439, 100)\n",
      "Test Shape :  (28000, 784)\n",
      "Val Shape  :  (3360, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Shape: \",train_images.shape)\n",
    "print(\"Test Shape : \",test_images.shape)\n",
    "print(\"Val Shape  : \",val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "honey-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 10, 10)\n",
    "val_images = val_images.reshape(val_images.shape[0], 10, 10)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "normal-rendering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (13439, 10, 10)\n",
      "Test Shape :  (28000, 28, 28)\n",
      "Val Shape  :  (3360, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Shape: \",train_images.shape)\n",
    "print(\"Test Shape : \",test_images.shape)\n",
    "print(\"Val Shape  : \",val_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-ideal",
   "metadata": {},
   "source": [
    "## Plotting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "yellow-impact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAABvCAYAAACOylkJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAHKUlEQVR4nO3dTWhdZR7H8d+/adO0pSq+gmmmdMbWZUVjhVIRGbS6kG4riIgjGdSAw6w64EJ8ARFxISLYhbixiiMWFHyrIBRx0KTq+FqdWlPazMKpDljwjdi/i1wx1fNPTuzznHPu4/ezae5zk/v8e3/0xzk99+aauwsA8GtL2h4AALqKggSAAAUJAAEKEgACFCQABChIAAhQkAAQKKYgzWzczCbN7Dsze6zteZAGuZapX3Jd2vYACf1X0t2Stkpa0fIsSIdcy9QXuRZTkO7+jCSZ2aikNS2Pg0TItUz9kmsxp9gAkBoFCQABChIAAhQkAASKuUhjZks1+/cZkDRgZkOSZtx9pt3JcDLItUz9kmtJR5C3S/pG0g5J1/W+vr3ViZACuZapL3I1fmEuAFQr6QgSAJKiIAEgQEECQICCBIBArYI0s6vM7GMzO2BmO3IPhWaQa5nINZ0Fr2Kb2YCkTyRdIemIpAlJ17r7h9HPDNpyH9KqlHP2lWP6/1F3P6vtOeZDrotHrmWaL9c6LxTfJOmAux+UJDN7UtI2SeETPqRVusT+/FtmLcIr/vShtmeogVwXiVzLNF+udU6xhyUdnnP7SG8N/Y1cy0SuCSV7q6GZjUkak6QhrUz1sGgZuZaJXOupcwQ5LWlkzu01vbUTuPtOdx9199FlWp5qPuRDrmUi14TqFOSEpPVmts7MBiVtl/Rs3rHQAHItE7kmtOAptrvPmNm4pJc0+5s3HnX3D7JPlsFXL/wpvO+Uqz9tcJL2lZQrfkauadX6P0h3f17S85lnQcPItUzkmg7vpAGAAAUJAAEKEgACFCQABChIAAgU86Fdddx//j/D++7UhQ1OgpSmd2wO7xu+9/UGJ8F8lqysfsfOtsnqt0I/+Pi28LFG7mkmV44gASBAQQJAgIIEgAAFCQABChIAAkVexb7rs4nK9b88fFv4M+eKq51dd/S5DZXra8cPV65L0kyuYbBox7/+unL9whVTlesje45lnKYejiABIEBBAkCAggSAAAUJAAEKEgACFCQABPr6ZT4Dp51aub5p+bLK9ZFH3g8f64ckEyEJs8rlfRc9Vbm+9dAFGYdBMkGu5y+rfjHWknf/Ez7U8SQDLYwjSAAIUJAAEKAgASBAQQJAoNZFGjObknRMs9cyZtx9NOdQaAa5lolc01nMVezL3f1otkl+g1sn36hcv3T8r5XrK7+q/v7fuc7lunT43Mr1y8Yurlwf0ps5x+lXncv1+63VPX3qkrcr149/+23OcWrhFBsAAnUL0iW9bGb7zGws50BoFLmWiVwTqXuKvcXdp83sbEl7zGy/u++d+w29IMYkaUjVn16GziHXMpFrIrWOIN19uvfn55J2S9pU8T073X3U3UeXaXnaKZEFuZaJXNNZsCDNbJWZrf7pa0lXSorfs4e+QK5lIte06pxinyNpt82+j3KppF3u/mLWqebwzRvD+zYOvla5vvIZrlbX0Gqu87nvtacr1/++bnPDk/Slzua6+h/VH43xwJd/bHiS+hYsSHc/KCluKfQlci0TuabFy3wAIEBBAkCAggSAAAUJAAEKEgACnf/IhYeeeDi878btt1aum/6daxwkMnDG6eF91+ytznW9v5VrHDTgluFXK9f/tuvGyvW1+lfOcWrhCBIAAhQkAAQoSAAIUJAAEKAgASDQ+avYN+2/Lrxvxetcre5XH919Xnjfhuv5CIUSbRz8onJ97R3dzZsjSAAIUJAAEKAgASBAQQJAgIIEgAAFCQCBzr/MZ8XWz9oeARlsuLm7L+1AHjf8YUtwzw+NzrEYHEECQICCBIAABQkAAQoSAAIUJAAEzN3TP6jZ/yQd6t08U9LR5JvU18b+a939rIb3zI5cybUBnco1S0GesIHZpLuPZt2kw/uXqu3nte39S9X289r2/r/EKTYABChIAAg0UZA7G9ijy/uXqu3nte39S9X289r2/ifI/n+QANCvOMUGgEC2gjSzq8zsYzM7YGY7cu0zz/5TZvaemb1jZpNN71+qtnPtzUC2GbSdbRdzzfU6yAFJn0i6QtIRSROSrnX3D5NvFs8wJWnU3dt8TVdRupBrb44pkW1SXci2i7nmOoLcJOmAux909+8lPSlpW6a90BxyLRfZVshVkMOSDs+5faS31iSX9LKZ7TOzsYb3LlUXcpXINocuZNu5XDv/C3NPwhZ3nzazsyXtMbP97r637aGQBNmWqXO55jqCnJY0Muf2mt5aY9x9uvfn55J2a/YUAien9Vwlss2k9Wy7mGuugpyQtN7M1pnZoKTtkp7NtNevmNkqM1v909eSrpT0flP7F6zVXCWyzYh/sxWynGK7+4yZjUt6SdKApEfd/YMcewXOkbTbzKTZv+Mud3+xwf2L1IFcJbLNogPZdjJX3kkDAAHeSQMAAQoSAAIUJAAEKEgACFCQABCgIAEgQEECQICCBIDAjxMx9SGv2dXSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- train samples\n",
    "for i in range(6, 9):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.title(train_labels[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floppy-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- train\n",
    "train_images_tensor = torch.from_numpy(train_images)/255.0\n",
    "train_labels_tensor = torch.from_numpy(train_labels)\n",
    "train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "\n",
    "# --- val\n",
    "val_images_tensor = torch.from_numpy(val_images)/255.0\n",
    "val_labels_tensor = torch.from_numpy(val_labels)\n",
    "val_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n",
    "\n",
    "# --- test\n",
    "test_images_tensor = torch.from_numpy(test_images)/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "demanding-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_tensor, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_tensor, batch_size=4, shuffle=True)\n",
    "test_loader  = DataLoader(test_images_tensor, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-montgomery",
   "metadata": {},
   "source": [
    "## Define the CNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "controlling-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # --- Define Conv Part\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # --- Define Classification Part\n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Linear(4*5*5, 4)\n",
    "        )\n",
    "        \n",
    "    # --- Define forward\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_block(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "imported-session",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv_block): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_block): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = ConvNet()\n",
    "conv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-wilson",
   "metadata": {},
   "source": [
    "## optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "automatic-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(params=conv_model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-ordinance",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "broadband-caution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 11, loss = 1.2799\n",
      "epoch 3 / 11, loss = 0.9152\n",
      "epoch 5 / 11, loss = 0.6998\n",
      "epoch 7 / 11, loss = 0.1736\n",
      "epoch 9 / 11, loss = 0.4332\n",
      "epoch 11 / 11, loss = 0.1933\n"
     ]
    }
   ],
   "source": [
    "# --- Training \n",
    "n_total_steps = len(train_loader)\n",
    "num_epochs = 11\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # --- make true data and target\n",
    "        temp_target = target.numpy()\n",
    "        result_target = []\n",
    "        for i in temp_target:\n",
    "            if i == 0:\n",
    "                result_target.append(0)\n",
    "            elif i == 1:\n",
    "                result_target.append(1)\n",
    "            elif i == 5:\n",
    "                result_target.append(2)\n",
    "            elif i == 9:\n",
    "                result_target.append(3)\n",
    "                \n",
    "        target = torch.tensor(result_target)\n",
    "        data = data.unsqueeze(1)\n",
    "        \n",
    "        # --- forward\n",
    "        output = conv_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # --- backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # --- some Info\n",
    "    if epoch%2 == 0:\n",
    "        print(f'epoch {epoch+1} / {num_epochs}, loss = {loss.item():.4f}')\n",
    "    loss_list.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incorporated-continent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Plot Loss \n",
    "x = range(num_epochs)\n",
    "plt.plot(x, loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "loose-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv_block): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_block): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- save the model\n",
    "# torch.save(conv_model.state_dict(), \"./models/model.pt\")\n",
    "\n",
    "# --- load the model\n",
    "conv_model.load_state_dict(torch.load(\"./models/model.pt\"))\n",
    "conv_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-concern",
   "metadata": {},
   "source": [
    "## Evaluate CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hydraulic-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Val Loss: 0.2602, Val Accuracy: 3140/3360 (93.452%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv_model.eval()\n",
    "loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        \n",
    "        # --- make true data and target\n",
    "        temp_target = target.numpy()\n",
    "        result_target = []\n",
    "        for i in temp_target:\n",
    "            if i == 0:\n",
    "                result_target.append(0)\n",
    "            elif i == 1:\n",
    "                result_target.append(1)\n",
    "            elif i == 5:\n",
    "                result_target.append(2)\n",
    "            elif i == 9:\n",
    "                result_target.append(3)\n",
    "                \n",
    "        target = torch.tensor(result_target)\n",
    "        data = data.unsqueeze(1)\n",
    "        output = conv_model(data)\n",
    "\n",
    "        loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss /= len(val_loader.dataset)\n",
    "\n",
    "    print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-extra",
   "metadata": {},
   "source": [
    "## Test Dataset and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "general-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal value   :  5\n",
      "Presicted value :  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKRElEQVR4nO3d26udd53H8fenOZp0rMIMDCZlGlA6kxFmWjadaqEDjTA6lXYu5qJCCyMD8cJDFKFUb/wHxNGLUgjR3ljai9gLkWIdPCBzE9xNCjZJHULVJD1gHGitRXOg37nYeyCTNFlPVtYzz97feb+gkL32yq8fSt591l5Ze+1UFZL6uGHqAZIWy6ilZoxaasaopWaMWmpm4xiHbs6W2sr2MY6WBPyRtzhXZ/NOnxsl6q1s5++yZ4yjJQGH6odX/JwPv6VmjFpqxqilZoxaasaopWaMWmpmUNRJPprkF0lOJHlk7FGS5jcz6iQbgEeBjwG7gU8k2T32MEnzGXKlvgM4UVUvVdU54Cng/nFnSZrXkKh3AKcu+vj06m3/S5K9SZaTLJ/n7KL2SbpGC3uirKr2V9VSVS1tYsuijpV0jYZE/TJw80Uf71y9TdIaNCTqnwEfSLIryWbgAeC7486SNK+Z36VVVReSfAZ4FtgAfKuqjo6+TNJcBn3rZVU9Azwz8hZJC+AryqRmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZgb9LC31t+Gvbx3l3EpGOXcMb7/w4tQTFsIrtdSMUUvNGLXUjFFLzRi11IxRS80YtdTMzKiT3Jzkx0mOJTmaZN//xTBJ8xny4pMLwBer6nCSPwGeS/LvVXVs5G2S5jDzSl1Vr1bV4dVfvwkcB3aMPUzSfK7pZaJJbgFuAw69w+f2AnsBtrJtEdskzWHwE2VJbgS+A3y+qn536eeran9VLVXV0ia2LHKjpGswKOokm1gJ+omqenrcSZKux5BnvwN8EzheVV8bf5Kk6zHkSn0X8BBwT5LnV//5x5F3SZrTzCfKquo/gPXzTbHS/3O+okxqxqilZoxaasaopWZ848GR/OGf7hjl3B8++tgo5/7ryXePcu7vz4/zQqQbN51d+Jmvn/vzhZ8JcPbvXxvl3CvxSi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNeO7iQJk8T9VaKx3/Tx54Q+jnPtfD9w0yrkXfn1qlHPfGuXUy35C87rklVpqxqilZoxaasaopWaMWmrGqKVmjFpqZnDUSTYkOZLke2MOknR9ruVKvQ84PtYQSYsxKOokO4F7gQPjzpF0vYZeqb8OPAy8faU7JNmbZDnJ8nkW/wPBJQ0zM+okHwd+U1XPXe1+VbW/qpaqamkTWxY2UNK1GXKlvgu4L8mvgKeAe5J8e9RVkuY2M+qq+lJV7ayqW4AHgB9V1YOjL5M0F/+eWmrmmr6fuqp+AvxklCWSFsIrtdSMUUvNGLXUjFFLzRi11IzvJgrcsGXxr4C7796HFn4mwA2v/36Uc//tp0+Ocu6n9u4b5dzNzy6Pcm4HXqmlZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWaMWmrGqKVmjFpqxqilZoxaasaopWZ8N1HgPw/81cLPfP+DRxZ+JkCN8M6nAAffuH2Uc8+9e8Mo524e5dQevFJLzRi11IxRS80YtdSMUUvNGLXUjFFLzQyKOsl7khxM8mKS40k+NPYwSfMZ+uKTbwDfr6p/TrIZ2DbiJknXYWbUSW4C7gb+BaCqzgHnxp0laV5DHn7vAs4Ajyc5kuRAku2X3inJ3iTLSZbPc3bhQyUNMyTqjcDtwGNVdRvwFvDIpXeqqv1VtVRVS5sY5/XJkmYbEvVp4HRVHVr9+CArkUtag2ZGXVWvAaeS3Lp60x7g2KirJM1t6LPfnwWeWH3m+yXgk+NNknQ9BkVdVc8DS+NOkbQIvqJMasaopWaMWmrGqKVmjFpqxncTBe5+/4mFn/n4K88v/EyAn/5xlGP58sOfGuXcGw8emn0nLZRXaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aaMWqpGaOWmjFqqRmjlpoxaqkZo5aa8Y0HgVfufHPhZ/4Df7vwM8e0Hd8gsAuv1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzg6JO8oUkR5O8kOTJJFvHHiZpPjOjTrID+BywVFUfBDYAD4w9TNJ8hj783gi8K8lGYBvwyniTJF2PmVFX1cvAV4GTwKvAG1X1g0vvl2RvkuUky+c5u/ilkgYZ8vD7vcD9wC7gfcD2JA9eer+q2l9VS1W1tIkti18qaZAhD78/Avyyqs5U1XngaeDD486SNK8hUZ8E7kyyLUmAPcDxcWdJmteQr6kPAQeBw8DPV3/P/pF3SZrToO+nrqqvAF8ZeYukBfAVZVIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM0YtNWPUUjNGLTVj1FIzRi01Y9RSM6mqxR+anAF+PeCufwr8duEDxrOe9q6nrbC+9q6FrX9RVX/2Tp8YJeqhkixX1dJkA67Retq7nrbC+tq71rf68FtqxqilZqaOer398Pr1tHc9bYX1tXdNb530a2pJizf1lVrSghm11MxkUSf5aJJfJDmR5JGpdsyS5OYkP05yLMnRJPum3jREkg1JjiT53tRbribJe5IcTPJikuNJPjT1pqtJ8oXVPwcvJHkyydapN11qkqiTbAAeBT4G7AY+kWT3FFsGuAB8sap2A3cCn17DWy+2Dzg+9YgBvgF8v6r+Evgb1vDmJDuAzwFLVfVBYAPwwLSrLjfVlfoO4ERVvVRV54CngPsn2nJVVfVqVR1e/fWbrPyh2zHtqqtLshO4Fzgw9ZarSXITcDfwTYCqOldVr086araNwLuSbAS2Aa9MvOcyU0W9Azh10cenWeOhACS5BbgNODTxlFm+DjwMvD3xjll2AWeAx1e/VDiQZPvUo66kql4GvgqcBF4F3qiqH0y76nI+UTZQkhuB7wCfr6rfTb3nSpJ8HPhNVT039ZYBNgK3A49V1W3AW8Bafn7lvaw8otwFvA/YnuTBaVddbqqoXwZuvujjnau3rUlJNrES9BNV9fTUe2a4C7gvya9Y+bLmniTfnnbSFZ0GTlfV/zzyOchK5GvVR4BfVtWZqjoPPA18eOJNl5kq6p8BH0iyK8lmVp5s+O5EW64qSVj5mu94VX1t6j2zVNWXqmpnVd3Cyn/XH1XVmruaAFTVa8CpJLeu3rQHODbhpFlOAncm2bb652IPa/CJvY1T/Eur6kKSzwDPsvIM4req6ugUWwa4C3gI+HmS51dv+3JVPTPdpFY+Czyx+j/3l4BPTrzniqrqUJKDwGFW/lbkCGvwJaO+TFRqxifKpGaMWmrGqKVmjFpqxqilZoxaasaopWb+G+R1LyGvpFuKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 29\n",
    "# --- plot image\n",
    "test_image = val_images[idx].astype(np.uint8)\n",
    "plt.imshow(test_image)\n",
    "\n",
    "# --- show orginal value\n",
    "print(\"Orginal value   : \", val_labels[idx])\n",
    "\n",
    "# --- show predicted value\n",
    "with torch.no_grad():\n",
    "    tensor_image = torch.from_numpy(val_images[idx])\n",
    "    tensor_image = tensor_image.unsqueeze(0)\n",
    "    tensor_image = tensor_image.unsqueeze(0)\n",
    "    predicted_value = conv_model(tensor_image).numpy()\n",
    "    predicted_value = np.argmax(predicted_value)\n",
    "    if predicted_value == 2:\n",
    "        predicted_value = 5\n",
    "    elif predicted_value == 3:\n",
    "        predicted_value = 9\n",
    "    print(\"Presicted value : \", predicted_value)    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx in range(len(val_labels)):\n",
    "        test_image = val_images[idx].astype(np.uint8)\n",
    "\n",
    "        tensor_image = torch.from_numpy(val_images[idx])\n",
    "        tensor_image = tensor_image.unsqueeze(0)\n",
    "        tensor_image = tensor_image.unsqueeze(0)\n",
    "        predicted_value = conv_model(tensor_image).numpy()\n",
    "        predicted_value = np.argmax(predicted_value)\n",
    "        if predicted_value == 2:\n",
    "            predicted_value = 5\n",
    "        elif predicted_value == 3:\n",
    "            predicted_value = 9\n",
    "\n",
    "        if val_labels[idx] != predicted_value:\n",
    "            print(idx)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-argument",
   "metadata": {},
   "source": [
    "## ONNX Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "built-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export the model\n",
    "x = torch.randn(4, 1, 10, 10, requires_grad=True)\n",
    "\n",
    "torch.onnx.export(conv_model,                # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"./models/model.onnx\",     # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']  # the model's output names\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "possible-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load and check\n",
    "onnx_model = onnx.load(\"./models/model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "musical-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import numpy_helper\n",
    "INTIALIZERS  = onnx_model.graph.initializer\n",
    "nodes=onnx_model.graph.node\n",
    "onnx_weights = {}\n",
    "for initializer in INTIALIZERS:\n",
    "    W = numpy_helper.to_array(initializer)\n",
    "    onnx_weights[initializer.name] = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "responsible-watts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conv_block.0.weight', 'conv_block.0.bias', 'linear_block.0.weight', 'linear_block.0.bias', '17'])\n"
     ]
    }
   ],
   "source": [
    "print(onnx_weights.keys())\n",
    "ky=list(onnx_weights.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "domestic-origin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08283696 -0.3559535  -0.13986284]\n",
      " [ 0.04469441 -0.42447752 -0.66592926]\n",
      " [ 0.3191516   0.64586574  0.88528347]]\n"
     ]
    }
   ],
   "source": [
    "conv_w_1 = onnx_weights['conv_block.0.weight'][0][0]\n",
    "print(conv_w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effective-search",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34157342 -0.25934675  0.11585948]\n",
      " [-0.12829612  0.0625388  -0.05002215]\n",
      " [-0.21764195  0.19251591 -0.26899686]]\n"
     ]
    }
   ],
   "source": [
    "conv_w_2 = onnx_weights['conv_block.0.weight'][1][0]\n",
    "print(conv_w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wanted-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20927872 -0.1884275   0.15589876]\n",
      " [-0.1748103  -0.05102624  0.21250954]\n",
      " [-0.13811535 -0.31749466 -0.05603961]]\n"
     ]
    }
   ],
   "source": [
    "conv_w_3 = onnx_weights['conv_block.0.weight'][2][0]\n",
    "print(conv_w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "royal-thickness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61532557  1.3386121   0.31283677]\n",
      " [ 1.0323175   1.2751912  -0.31333235]\n",
      " [ 1.4164771   0.20937458  0.11632154]]\n"
     ]
    }
   ],
   "source": [
    "conv_w_4 = onnx_weights['conv_block.0.weight'][3][0]\n",
    "print(conv_w_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "political-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.2623779e-01 -2.5713012e-01  2.0757751e-01  4.5666384e-04]\n"
     ]
    }
   ],
   "source": [
    "conv_b = onnx_weights['conv_block.0.bias']\n",
    "print(conv_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "geographic-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-18.  18.   0.  -6.]\n"
     ]
    }
   ],
   "source": [
    "conv_b = onnx_weights['linear_block.0.bias']\n",
    "print(np.round(conv_b*64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "australian-flesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030218\n",
      "1.0476896\n",
      "0.6279603\n",
      "0.7843218\n"
     ]
    }
   ],
   "source": [
    "print(np.max(abs(onnx_weights['linear_block.0.weight'][0])))\n",
    "print(np.max(abs(onnx_weights['linear_block.0.weight'][1])))\n",
    "print(np.max(abs(onnx_weights['linear_block.0.weight'][2])))\n",
    "print(np.max(abs(onnx_weights['linear_block.0.weight'][3])))                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-funeral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bound-rugby",
   "metadata": {},
   "source": [
    "## Quantizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "tired-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Quntizing Conv layer ---------\n",
    "# --- Conv weights --> (S.1.6)\n",
    "# --- Conv biases  --> (S.0.7)\n",
    "# --- input        --> (S.0.7)\n",
    "# --- W*I          --> (S.5.13)\n",
    "# --- W*I + b      --> (S.6.7)  \n",
    "\n",
    "# ---------- Quantizing Classifiation Layer ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "radical-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -5. -23.  -9.]\n",
      " [  3. -27. -43.]\n",
      " [ 20.  41.  57.]]\n"
     ]
    }
   ],
   "source": [
    "qconv_w_1 = np.round(conv_w_1*64) \n",
    "print(qconv_w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "emotional-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22. -17.   7.]\n",
      " [ -8.   4.  -3.]\n",
      " [-14.  12. -17.]]\n"
     ]
    }
   ],
   "source": [
    "qconv_w_2 = np.round(conv_w_2*64) \n",
    "print(qconv_w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mineral-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-13. -12.  10.]\n",
      " [-11.  -3.  14.]\n",
      " [ -9. -20.  -4.]]\n"
     ]
    }
   ],
   "source": [
    "qconv_w_3 = np.round(conv_w_3*64) \n",
    "print(qconv_w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "multiple-violin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39.  86.  20.]\n",
      " [ 66.  82. -20.]\n",
      " [ 91.  13.   7.]]\n"
     ]
    }
   ],
   "source": [
    "qconv_w_4 = np.round(conv_w_4*64) \n",
    "print(qconv_w_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "casual-right",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 67. -33.  27.   0.]\n"
     ]
    }
   ],
   "source": [
    "qconv_b = np.round(conv_b*128) \n",
    "print(qconv_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "understanding-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quantizing Classificatin Weights\n",
    "qclass_w_1 = np.round(onnx_weights['linear_block.0.weight'][0] * 64)\n",
    "qclass_w_2 = np.round(onnx_weights['linear_block.0.weight'][1] * 64)\n",
    "qclass_w_3 = np.round(onnx_weights['linear_block.0.weight'][2] * 64)\n",
    "qclass_w_4 = np.round(onnx_weights['linear_block.0.weight'][3] * 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cognitive-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make binary of concatenated data to write them in BRAM file\n",
    "qclass_w = []\n",
    "for i in range(0,25):\n",
    "    temp =  Bits(int=int(qclass_w_1[i]),length=8).bin    + \\\n",
    "            Bits(int=int(qclass_w_2[i]),length=8).bin    + \\\n",
    "            Bits(int=int(qclass_w_3[i]),length=8).bin    + \\\n",
    "            Bits(int=int(qclass_w_4[i]),length=8).bin    + \\\n",
    "            Bits(int=int(qclass_w_1[i+25]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_2[i+25]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_3[i+25]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_4[i+25]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_1[i+50]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_2[i+50]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_3[i+50]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_4[i+50]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_1[i+75]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_2[i+75]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_3[i+75]),length=8).bin + \\\n",
    "            Bits(int=int(qclass_w_4[i+75]),length=8).bin\n",
    "    qclass_w.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dense-radical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round((onnx_weights['linear_block.0.weight'][0][0])* 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "liquid-discovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04208606"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_weights['linear_block.0.weight'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "clean-franklin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.046875"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-384/8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "three-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Insert data in .COE BRAM initialization file\n",
    "f = open(\"./inc/class_BRAM.coe\", \"w\")\n",
    "f.write('memory_initialization_radix = 2;')\n",
    "f.write(\"\\n\")\n",
    "f.write('memory_initialization_vector =')\n",
    "f.write(\"\\n\")\n",
    "for index,data in enumerate(qclass_w):\n",
    "    f.write(data)\n",
    "    if index == len(qclass_w)-1:\n",
    "        f.write(';')\n",
    "    else:\n",
    "        f.write(',')\n",
    "    f.write(\"\\n\")\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-disco",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-breakdown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-horizon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "physical-lotus",
   "metadata": {},
   "source": [
    "## Test An Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mature-shell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13439, 10, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "recent-details",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Value is:  1\n",
      "Predicted Value is:  tensor([[-2.9169,  3.6037,  0.1704, -0.2773]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_image = train_images[0]\n",
    "test_tensor = torch.from_numpy(test_image/255)\n",
    "test_tensor = test_tensor.unsqueeze(0)\n",
    "test_tensor = test_tensor.unsqueeze(0)\n",
    "test_tensor.shape\n",
    "print(\"Orginal Value is: \", train_labels[0])\n",
    "print(\"Predicted Value is: \",conv_model(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "architectural-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Value is:  1\n",
      "Predicted Value is:  tensor([[ 3.5500, -2.6531,  1.3534, -1.3254]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_image = train_images[500]\n",
    "test_tensor = torch.from_numpy(test_image/255)\n",
    "test_tensor = test_tensor.unsqueeze(0)\n",
    "test_tensor = test_tensor.unsqueeze(0)\n",
    "test_tensor.shape\n",
    "print(\"Orginal Value is: \", train_labels[0])\n",
    "print(\"Predicted Value is: \",conv_model(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "technical-cliff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 227., -170.,   87.,  -85.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.array([ 3.5500, -2.6531,  1.3534, -1.3254]) * 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "amino-blackjack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.059375,  0.012475, -0.087775, -0.002725])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([231, -169, 81, -85]) / 64) - np.array([ 3.5500, -2.6531,  1.3534, -1.3254])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fiscal-moisture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.60492024, -2.64144668,  1.26697331, -1.32941756])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ 3.890625, -2.921875,  1.265625, -1.234375]) + onnx_weights['linear_block.0.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "generic-carpet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18.285105  ,  17.947412  ,   0.08629173,  -6.0827236 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_weights['linear_block.0.bias']*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fewer-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1.31 us\n",
    "# --- 996  us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "passing-substitute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   4., 165.,   3.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 145., 251.,  56.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 225., 253.,   4.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 248., 242.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   6., 251., 223.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   9., 251., 223.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   1., 251., 248.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 150., 163.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equivalent-classification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 242., 233., 234.,   2.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,  28., 247.,   0.,   7., 116.,   0.,   0.],\n",
       "       [  0.,   0.,   0., 118.,   0.,   0.,   0., 186.,   0.,   0.],\n",
       "       [  0.,   0.,   0., 230.,   0.,   0.,   0., 193.,   0.,   0.],\n",
       "       [  0.,   0.,   0., 215.,   0.,   0.,   0., 140.,   0.,   0.],\n",
       "       [  0.,   0.,   0., 206.,   0.,   0.,  63.,  72.,   0.,   0.],\n",
       "       [  0.,   0.,   0., 169.,  75., 128., 168.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "vulnerable-applicant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   3.,  83.,   2.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  73., 126.,  29.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 113., 127.,   3.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0., 125., 122.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   4., 126., 112.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   5., 126., 112.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   1., 126., 125.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  76.,  82.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil((test_image/255)*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "editorial-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make an stream data from normalized image\n",
    "f = open(\"./inc/image_stream.txt\", \"w\")\n",
    "for i in test_image:\n",
    "    for j in i:\n",
    "        if j == 255:\n",
    "            j = 254\n",
    "        j = math.ceil(j/255 * 128)\n",
    "        f.write(str(j))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "for i in range(17):\n",
    "    f.write(str(0))\n",
    "    f.write(\"\\n\")\n",
    "        \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sealed-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Make an stream data from normalized image\n",
    "f = open(\"./inc/image_stream2.txt\", \"w\")\n",
    "for i in test_image:\n",
    "    for j in i:\n",
    "        if j == 255:\n",
    "            j = 254\n",
    "        j = math.ceil(j/255 * 128)\n",
    "        f.write(str(j))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "for i in range(17):\n",
    "    f.write(str(0))\n",
    "    f.write(\"\\n\")\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "tamil-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 67  67  67  69 142 122  94  67  67  67]\n",
      " [ 67  67  67 130 168 160 128  76  67  67]\n",
      " [ 67  67  67 118 124  86  95  69  67  67]\n",
      " [ 67  67  67  92  78  78  89  64  67  67]\n",
      " [ 67  67  70  81  55  77  96  66  67  67]\n",
      " [ 67  67  68  78  58  83  97  67  67  67]\n",
      " [ 67  67  63  73  69  95 102  67  67  67]\n",
      " [ 67  67  65  30 -10  46  89  67  67  67]\n",
      " [ 67  67  66  -3 -84 -19  61  67  67  67]\n",
      " [ 67  67  67  56  28  31  60  67  67  67]]\n"
     ]
    }
   ],
   "source": [
    "# --- recover stream result from FPGA\n",
    "file1 = open('./inc/result_conv.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "data = []\n",
    "for index,line in enumerate(Lines):\n",
    "    if index>=19 and index<119:\n",
    "        data.append(int(line))\n",
    "\n",
    "file1.close()\n",
    "data = np.array(data)\n",
    "data = data.reshape(10,10)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "frank-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  27.   27.   27.   27.   22.   -0.   15.   27.   27.   27.]\n",
      " [  27.   27.   27.   23.   14.  -29.  -14.   23.   27.   27.]\n",
      " [  27.   27.   27.   37.   20.  -58.  -32.   22.   27.   27.]\n",
      " [  27.   27.   27.   55.    8.  -89.  -44.   21.   27.   27.]\n",
      " [  27.   27.   27.   63.   -2. -101.  -37.   27.   27.   27.]\n",
      " [  27.   27.   27.   65.   -8. -103.  -33.   27.   27.   27.]\n",
      " [  27.   27.   28.   65.  -10. -105.  -33.   27.   27.   27.]\n",
      " [  27.   27.   28.   68.   11.  -85.  -29.   27.   27.   27.]\n",
      " [  27.   27.   27.   63.   36.  -40.  -13.   27.   27.   27.]\n",
      " [  27.   27.   27.   39.   26.   -4.   10.   27.   27.   27.]]\n"
     ]
    }
   ],
   "source": [
    "conved = functions.convolution2d(test_image/255, conv_w_3, 1, 1) + conv_b[2]\n",
    "print(np.ceil(conved*128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "coupled-spectacular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-32.912654876708984"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_b[1] * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "other-reputation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.569921493530273"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_b[2] * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-italian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
